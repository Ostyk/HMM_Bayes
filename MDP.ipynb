{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Processes (10 pts, 3 bonus)\n",
    "\n",
    "Assignment:\n",
    "* The environment is FULLY observable.\n",
    "* No graphical rendering is needed.\n",
    "* Use ONLY discrete states (i.e., instead of images, you will return the real\n",
    "environment state as observation).\n",
    "* Use a reduced state space (e.g., the state of the grid should be small - at most\n",
    "$10\\times10$). Adapt the number of walls, ghosts or invaders accordingly.\n",
    "* Use ONLY discrete actions.\n",
    "* Except the agent (i.e., the player) everything is STATIC.\n",
    "* Write a simple test program where you create an instance of the enironment and you\n",
    "execute a sequence of at least 5 random actions, printing (or showing in any form\n",
    "you prefer) the observation you get out of it.\n",
    "- Bonus Points (3 pts): Expose the transition model from the environment to the agent, implement the value iteration algorithm and run it on your environment. '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each time step, the agent performs an action which leads to two things: changing the environment state and the agent (possibly) receiving a reward (or penalty) from the environment. The goal of the agent is to discover an optimal policy (i.e. what actions to do in each state) such that it maximizes the total value of rewards received from the environment in response to its actions. MDPis used to describe the agent/ environment interaction settings in a formal way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP consits of a tuple of 5 elements:\n",
    "* $S -$ set of states. At each time step the state of the enviropment is an element $s âˆˆ S.$\n",
    "* $A -$ set of actions. At each time step the agent choses an action $a âˆˆ A$ to perform.\n",
    "* $T = p(s_{t+1} | s_t, a_t)$ $-$ (State) Transition model, which depicts how the enivorments states changes when the agent performs an action $a$ depending on the $a$ and the current state $s$.\n",
    "* $R(s) = p(r_{t+1} | s_t, a_t)$ $-$ Reward model that depicts the real-valued reward value that the agent receives from the enviroment after performing an action. Values depend on the current state and action performed.\n",
    "* $\\gamma $ $-$ discount factor that controls the importance of future awards. Where $0 \\leq \\gamma \\leq 1$. The reason for using discount factor is to prevent the total reward from going to infinity. Controls immidiete vs long-term reward.\n",
    "\n",
    "Remember that MDPs are: __fully observable__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fixed action sequence does __NOT__ solve the problem, as an agent might end in a different state from the goal. Hence, we must specify what the agent should do for any state in which it finds itself. This solution is called the agent __policy__ $\\pi$ :\n",
    "> $\\pi(s): S \\rightarrow A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There two types of enviroments:\n",
    "* A __Deterministic enviroment__, where both the state transition model $T$ and reward model $R(s)$ are deterministic functions. If the agent while in a given state repeats a given action, it will always go to the __same next state__ and receive the __same reward value__.\n",
    "* A __Stochastic environment__, where there is uncertainty about the actions effect. When the agent repeats doing the same action in a given state, the new state and received reward may not be the same each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation\n",
    "Bellman equation using dynamic programming paradigm provides a recursive definition for the optimal Q-function.\n",
    "* A relation between utility of a state and utility of its neighbors\n",
    "\n",
    "> $ U(s) = R(s) + \\gamma \\max\\limits_{a \\in A} \\sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) U(s_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration algorithm\n",
    "Assumption: the agent knows the MDP model of the world (i.e. the agent knows the state-transition and reward probability functions) $\\rightarrow$ can be used by the agent (offline) to plan its action given  knowledge about the enviroment before interacting with it.\n",
    "\n",
    "* n possible statesâ†’n Bellman equations (one per state)â†’n unknowns\n",
    "* Equations are nonlinear (max operation is nonlinear)\n",
    "* Solution is hard, but we can use iterative approaches\n",
    "\n",
    "Overview:\n",
    "\n",
    "1. Start with arbitrary initial values for utilities\n",
    "2. Until we reach equilibrium:\n",
    "    - For each state simultaneously:\n",
    "    $ U_{i+1} \\leftarrow R(s) + \\gamma \\max\\limits_{a \\in A} \\sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) U(s_{t+1})$\n",
    "    - Equilibrium is guaranted to be reached, and this is the solution to the equations\n",
    "    - Solutions ar eunique and the corresponding policy is optimal\n",
    "\n",
    "\n",
    "### Value (utility) function\n",
    "The value function represents how good is a state for an agent to be in. \n",
    "\n",
    "It is equal to expected total reward for an agent starting from state $s$. The value function depends on the policy by which the agent picks actions to perform. So, if the agent uses a given policy $\\pi$ to select actions, the corresponding value function is given by:\n",
    "> $U^\\pi(s) = E [\\sum_{T}^{i=1} \\gamma^{i-1} r_i] \\forall s \\in S$\n",
    "\n",
    "> Among all possible value-functions, there exist an optimal value function that has higher value than other functions for all states.\n",
    "\n",
    "> $U^{*}(s) = \\max\\limits_{\\pi} U^{\\pi}(s)$   $ \\forall s \\in S$\n",
    "\n",
    "> The optimal policy ð›‘* is the policy that corresponds to optimal value function.\n",
    "\n",
    "> $\\pi^{*} = \\arg \\max\\limits_{\\pi} U^{\\pi}(s)$   $\\forall s \\in S$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-12-19 03:11:59,328] Making new env: breakout-simple-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL\n",
      "X X X X X X X X X X X X\n",
      "X Â·     Â· G Â· G X Â· G X\n",
      "X X Â· X Â· Â· G Â· Â· Â· Â· X\n",
      "X   X Â· Â·   Â·   Â· X Â· X\n",
      "X Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· X\n",
      "X Â· Â· Â· X Â· Â·   X X X X\n",
      "X   Â· X X Â· Â· Â· X X X X\n",
      "X       Â· Â·     Â· X X X\n",
      "X Â·     Â· Â·       Â· X X\n",
      "X         Â·     X X á—§ X\n",
      "X Â· X     Â·   Â· X Â·   X\n",
      "X X X X X X X X X X X X\n",
      "________________________________________\n",
      "________________________________________\n",
      "MOVE: UP\n",
      "\n",
      "X X X X X X X X X X X X\n",
      "X Â·     Â· G Â· G X Â· G X\n",
      "X X Â· X Â· Â· G Â· Â· Â· Â· X\n",
      "X   X Â· Â·   Â·   Â· X Â· X\n",
      "X Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· X\n",
      "X Â· Â· Â· X Â· Â·   X X X X\n",
      "X   Â· X X Â· Â· Â· X X X X\n",
      "X       Â· Â·     Â· X X X\n",
      "X Â·     Â· Â·       Â· X X\n",
      "X         Â·     X X á—§ X\n",
      "X Â· X     Â·   Â· X Â·   X\n",
      "X X X X X X X X X X X X\n",
      "total reward: 1\n",
      "________________________________________\n",
      "MOVE: RIGHT\n",
      "\n",
      "X X X X X X X X X X X X\n",
      "X Â·     Â· G Â· G X Â· G X\n",
      "X X Â· X Â· Â· G Â· Â· Â· Â· X\n",
      "X   X Â· Â·   Â·   Â· X Â· X\n",
      "X Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· X\n",
      "X Â· Â· Â· X Â· Â·   X X X X\n",
      "X   Â· X X Â· Â· Â· X X X X\n",
      "X       Â· Â·     Â· X X X\n",
      "X Â·     Â· Â·       Â· X X\n",
      "X         Â·     X X á—§ X\n",
      "X Â· X     Â·   Â· X Â·   X\n",
      "X X X X X X X X X X X X\n",
      "total reward: 2\n",
      "________________________________________\n",
      "MOVE: DOWN\n",
      "\n",
      "X X X X X X X X X X X X\n",
      "X Â·     Â· G Â· G X Â· G X\n",
      "X X Â· X Â· Â· G Â· Â· Â· Â· X\n",
      "X   X Â· Â·   Â·   Â· X Â· X\n",
      "X Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· X\n",
      "X Â· Â· Â· X Â· Â·   X X X X\n",
      "X   Â· X X Â· Â· Â· X X X X\n",
      "X       Â· Â·     Â· X X X\n",
      "X Â·     Â· Â·       Â· X X\n",
      "X         Â·     X X á—§ X\n",
      "X Â· X     Â·   Â· X Â·   X\n",
      "X X X X X X X X X X X X\n",
      "total reward: 2\n",
      "________________________________________\n",
      "MOVE: UP\n",
      "\n",
      "X X X X X X X X X X X X\n",
      "X Â·     Â· G Â· G X Â· G X\n",
      "X X Â· X Â· Â· G Â· Â· Â· Â· X\n",
      "X   X Â· Â·   Â·   Â· X Â· X\n",
      "X Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· X\n",
      "X Â· Â· Â· X Â· Â·   X X X X\n",
      "X   Â· X X Â· Â· Â· X X X X\n",
      "X       Â· Â·     Â· X X X\n",
      "X Â·     Â· Â·       Â· X X\n",
      "X         Â·     X X á—§ X\n",
      "X Â· X     Â·   Â· X Â·   X\n",
      "X X X X X X X X X X X X\n",
      "total reward: 3\n",
      "________________________________________\n",
      "MOVE: RIGHT\n",
      "\n",
      "X X X X X X X X X X X X\n",
      "X Â·     Â· G Â· G X Â· G X\n",
      "X X Â· X Â· Â· G Â· Â· Â· Â· X\n",
      "X   X Â· Â·   Â·   Â· X Â· X\n",
      "X Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· X\n",
      "X Â· Â· Â· X Â· Â·   X X X X\n",
      "X   Â· X X Â· Â· Â· X X X X\n",
      "X       Â· Â·     Â· X X X\n",
      "X Â·     Â· Â·       Â· X X\n",
      "X         Â·     X X á—§ X\n",
      "X Â· X     Â·   Â· X Â·   X\n",
      "X X X X X X X X X X X X\n",
      "total reward: 4\n",
      "________________________________________\n",
      "MOVE: RIGHT\n",
      "\n",
      "X X X X X X X X X X X X\n",
      "X Â·     Â· G Â· G X Â· G X\n",
      "X X Â· X Â· Â· G Â· Â· Â· Â· X\n",
      "X   X Â· Â·   Â·   Â· X Â· X\n",
      "X Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· X\n",
      "X Â· Â· Â· X Â· Â·   X X X X\n",
      "X   Â· X X Â· Â· Â· X X X X\n",
      "X       Â· Â·     Â· X X X\n",
      "X Â·     Â· Â·       Â· X X\n",
      "X         Â·     X X á—§ X\n",
      "X Â· X     Â·   Â· X Â·   X\n",
      "X X X X X X X X X X X X\n",
      "total reward: 5\n",
      "________________________________________\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "if not os.path.split(os.getcwd())[1]=='breakout-simple': \n",
    "    os.chdir('breakout-simple')\n",
    "import breakout_simple\n",
    "\n",
    "env = gym.make('breakout-simple-v0')\n",
    "\n",
    "r=0\n",
    "print(\"INITIAL\")\n",
    "env.render()\n",
    "print(\"_\"*40)\n",
    "print(\"_\"*40)\n",
    "while r<5:\n",
    "    o, rew = env._step(env.action_space.sample())\n",
    "    if rew is not None:\n",
    "        r+=1\n",
    "    env.render()\n",
    "    print(\"total reward:\", r)\n",
    "    print(\"_\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+---------+\n",
    "|R: | : :G|\n",
    "| : : : : |\n",
    "| : : : : |\n",
    "| | : | : |\n",
    "|Y| : |B: |\n",
    "+---------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximize your score in the Atari 2600 game Breakout. In this environment, the observation is an RGB image of the screen, which is an array of shape $(210, 160, 3)$ Each action is repeatedly performed for a duration of kk frames, where kk is uniformly sampled from $\\{2, 3, 4\\}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import gym\n",
    "\n",
    "from matplotlib import animation\n",
    "from JSAnimation.IPython_display import display_animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment and display the initial state\n",
    "env = gym.make('Breakout-v0')\n",
    "observation = env.reset()\n",
    "firstframe = env.render(mode = 'rgb_array')\n",
    "fig,ax = plt.subplots()\n",
    "im = ax.imshow(firstframe) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "observation = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frames_as_gif(frames, filename_gif = None):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    if filename_gif: \n",
    "        anim.save(filename_gif, writer = 'imagemagick', fps=20)\n",
    "    display(display_animation(anim, default_mode='loop'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "frames = []\n",
    "for _ in range(5):\n",
    "    observation, reward, done, info = env.step(env.action_space.sample())\n",
    "    print(observation.shape, reward, done, info)\n",
    "#     frame = env.render(mode = 'rgb_array')\n",
    "#     im.set_data(frame)\n",
    "#     frames.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## displaying random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_frames_as_gif(frames, filename_gif='THIStest.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def create_action_space(size):\n",
    "    '''\n",
    "    for pretty printing-keep size less than 10.\n",
    "    '''\n",
    "    n_blocks = 2\n",
    "    bar = np.random.randint(0,size-1)\n",
    "    as_ = np.empty(shape=[size, size], dtype='<U1')\n",
    "    as_[:] = ' '\n",
    "    N_ghosts, N_walls, N_reward = int(1*size), int(4*size), int(7*size)\n",
    "    sum_ = N_ghosts + N_reward + N_walls\n",
    "    pos = lambda x: (np.random.randint(x), np.random.randint(x))\n",
    "    elem, count = [], 1\n",
    "    while len(elem)<=sum_+1:\n",
    "        sample = pos(size)\n",
    "        if sample not in elem:\n",
    "            s = pos(size)\n",
    "            elem.append(s)\n",
    "            if count<=N_ghosts: as_[s] = 'G' #ghosts\n",
    "            if N_ghosts < count <= N_ghosts + N_walls: as_[s] = 'X' #walls\n",
    "            if count > N_ghosts + N_walls: as_[s] = '\\u00B7'  # reward\n",
    "            if count == sum_+2: as_[s] = '\\u15E7'\n",
    "            count+=1\n",
    "    return as_\n",
    "    \n",
    "def pretty_print(as_):\n",
    "    size = as_.shape[0]\n",
    "    y=np.empty((size+2,size+2), dtype='<U1')\n",
    "    as_[as_ == ''] = ' '\n",
    "    y[:]='X'\n",
    "    y[1:size+1,1:size+1]=as_\n",
    "    pp = lambda x: print(re.sub('[\\'[\\]]', '', np.array_str(x)))\n",
    "    #for i in y: pp(i)\n",
    "        \n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "q=create_action_space(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.where(q=='\\u15E7'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q[7,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def create_action_space(self):\n",
    "    '''\n",
    "    for pretty printing-keep size less than 10.\n",
    "    '''\n",
    "    n_blocks = 2\n",
    "    bar = np.random.randint(0,self.size-1)\n",
    "    as_ = np.empty(shape=[self.size, self.size], dtype='<U1')\n",
    "    as_[0:n_blocks] = '-'\n",
    "    as_[self.size-1][bar] = 'B'\n",
    "    return as_\n",
    "\n",
    "def pretty_print(self):\n",
    "    pp = lambda x: print(re.sub('[\\'[\\]]', '', np.array_str(x)))\n",
    "    for i in self.action_space: pp(i)\n",
    "    e=np.empty(shape=[1, self.size], dtype='<U1')\n",
    "    e[:] = \"_\"\n",
    "    pp(e)\n",
    "    pp(np.arange(1,self.size+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q[1,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {0:\"UP\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
